{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "![](task1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Early stop triggers after 33 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "First, notice how the spikes happen every 200 steps. This corresponds to the number of steps required to complete an epoch of training. So what happens is that the weights for the last samples of the dataset are very different from the weights of the firsts samples, which can be explained by some ordering in the data set, yielding very different expected results depending on which batch is considered instead of which picture. For example, all the samples of target value \"2\" can be grouped at the beginning, and all the samples with target values \"3\" can be grouped at the end, yielding very different weights for the beginning and ending of the dataset. After shuffling, there should not be any correlation induced by the order of the samples anymore \n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "Overfitting can be detected by looking at the difference between validation and training accuracy. When this happens, the training accuracy continues growing while the validation accuracy decreases. Here, although the validation accuracy starts growing slower than the training accuracy, it still grows, so overfitting is probably not happening. Note that early stopping is implemented, and since its designed to stop overfitting, it make sense that we do not get to the point where it would have been observable on the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "![](task4a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "![](task4b_softmax_weight.png)  \n",
    "The noise can be created by weights set to unnecessarily high values to account for the noise of the samples. Without any regularization, if those wheights do not create an error with the target value for other samples, they will just be left as is by the gradient descent.  \n",
    "Whith the regularization, even if they do not create any error, they will be put down to 0, creating a uniform background of 0 valued weights, thus reducing visual noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "With the penalty, it is more difficult for the model weights to converge to the minimum of the cross entropy cost because the regularization term make each step bigger. Due to this increased difficulty, it could be like each sample contributes less to the model and thus more are needed for it to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4d_l2_reg_norms.png)\n",
    "\n",
    "We can see that the more the lambda is, the less the norm of the weights will be. This is quite expected as the regularization term matters more with a greater lambda, and all it does is trying to reduce the weights norms as much as possible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
