{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "![](task1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.b\n",
    "![](task1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) \n",
    "Training Set\n",
    "    mean = 33.318421449829934  \n",
    "    std = 78.56748998339798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "We add one to the input for each bias\n",
    "$785*64 + 65*10 = 50890$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "We see that the weight initialization improved the network accuracy significantly.\n",
    "Adding the improved sigmoid led to slightly better accuracy, but the momentum didn't further improve accuracy. \n",
    "We observe the same pattern with the loss. \n",
    "Convergence speed is measured by how quickly the early stopping quicks in. \n",
    "We see that the improved weight initialization did not lead to faster convergence. However, the improved sigmoid significantly sped up learning and momentum slighty improved on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a) and b) - Plot\n",
    "\n",
    "![](task4ab.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a) and b) - Comment\n",
    "We see that 32 nodes gives the worst results and 128 gives the best. The same apply to the loss. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "Original $785*64 + 65*10 = 50890$\n",
    "\n",
    "New $785*59 + 60*59 + 60*10 = 50455$\n",
    "\n",
    "784 -> 59 -> 59 -> 10, so there are 118 hidden units  \n",
    "Find below the graph of accuracy and loss of the base model, the model for 4d and the model for 4e  \n",
    "\n",
    "![](task4de.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d) and e) - Comment\n",
    "The model with 2 hidden layers (orange line) performs approximately the same as the base model from task 3.  \n",
    "With the 10 hidden layers network, we notice a deterioration of the network's performances, accuracy, loss, convergence speed are worse and the validation/loss variance is higher. \n",
    "This might be explained by the vanishing gradient problem, from [Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) :  \n",
    "```The problem is that as the sequence length increases, the gradient magnitude typically is expected to decrease (or grow uncontrollably), slowing the training process.```  \n",
    "So the network has higher variance from the big gradients terms, and learns more slowly from the small gradient terms.   \n",
    "Aditionnaly, the increased complexity of the network may lead to earlier overfitting, also preventing the model from learning quickly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
